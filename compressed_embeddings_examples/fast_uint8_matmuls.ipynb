{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc2a0890-910e-4dca-9471-f80974931ab3",
   "metadata": {},
   "source": [
    "# This is a playground notebook\n",
    "\n",
    "In order to evaluate int4 compression, we spent some time trying to get a decently fast matrix multiply operation implemented using `numpy` and `numba`. Unfortunately it was quite hard to compete with the performance of built-in `matmul` in `numpy` due to that function relying on very well optimized BLAS routines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "161b7bc5-a006-4c04-854f-89fa01680f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dd83754-653d-4fd6-aaf9-138f87513461",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython --compile-args=-fopenmp --link-args=-fopenmp\n",
    "cimport cython\n",
    "from cython.parallel import prange\n",
    "\n",
    "@cython.boundscheck(False)\n",
    "@cython.wraparound(False)\n",
    "def cython_matmul(cython.uint[:, ::1] out, cython.char[:, ::1] a, cython.char[::1, :] b, cython.long n, cython.long m, cython.long d):\n",
    "    cdef cython.uint tmp\n",
    "    cdef cython.int i, j, k\n",
    "    for i in prange(n, nogil=True):\n",
    "        for j in range(m):\n",
    "            tmp = 0\n",
    "            for k in range(d):\n",
    "                tmp = tmp + a[i, k] * b[k, j]\n",
    "            out[i, j] = out[i, j] + tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce61c9f1-5404-4930-9e3d-efce115a6271",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import cpu_count\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from collections import deque\n",
    "\n",
    "import numba\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "jit = numba.njit(error_model=\"numpy\", fastmath=True)\n",
    "ks = 16  # Kernel size.\n",
    "\n",
    "# Test data.\n",
    "n, m, d = 256, 4 * 4096, 256\n",
    "rng = np.random.default_rng(0)\n",
    "a = rng.choice(15, (n, d)).astype(np.uint8)\n",
    "b = rng.choice(15, (m, d)).astype(np.uint8).T\n",
    "\n",
    "def uint8_matmul_cython(a: NDArray[np.uint8], b: NDArray[np.uint8]) -> NDArray[np.uint32]:\n",
    "    n, d = a.shape\n",
    "    d2, m = b.shape\n",
    "    assert d == d2\n",
    "    assert a.flags.c_contiguous\n",
    "    assert b.flags.f_contiguous\n",
    "    out = np.zeros((n, m), dtype=np.uint32)\n",
    "    cython_matmul(out, a, b, n, m, d)\n",
    "    return out\n",
    "\n",
    "@jit\n",
    "def uint8_matmul_numba(a: NDArray[np.uint8], b: NDArray[np.uint8]) -> NDArray[np.uint32]:\n",
    "    \"\"\"Optimized multi-threaded implementation of matmul between uint4-stored-in-uint8 values.\"\"\"\n",
    "    n, d = a.shape\n",
    "    d2, m = b.shape\n",
    "    assert d2 == d\n",
    "    assert (a < 16).all(), \"Large value will trigger multiplication overlfow\"\n",
    "    assert (b < 16).all(), \"Large value will trigger multiplication overlfow\"\n",
    "    out = np.empty((n, m), dtype=np.uint32)\n",
    "    for i in range(n):\n",
    "        row = a[i, :]\n",
    "        for j in range(m):\n",
    "            col = b[:, j]\n",
    "            tmp = np.uint32(0)\n",
    "            for k in range(d):\n",
    "                tmp += row[k] * col[k]\n",
    "            out[i, j] = tmp\n",
    "    return out\n",
    "\n",
    "def uint8_matmul_einsum(a: NDArray[np.uint8], b: NDArray[np.uint8]) -> NDArray[np.uint32]:\n",
    "    return np.einsum(\"ij, jk -> ik\", a, b, dtype=np.uint32)\n",
    "\n",
    "@jit\n",
    "def _mm_uint8_kernel(out: NDArray[np.uint32], a: NDArray[np.uint8], b: NDArray[np.uint8]) -> NDArray[np.uint32]:\n",
    "    for i in range(ks):\n",
    "        row = a[i, :]\n",
    "        for j in range(ks):\n",
    "            col = b[:, j]\n",
    "            tmp = np.uint32(0)\n",
    "            for k in range(ks):\n",
    "                tmp += row[k] * col[k]\n",
    "            out[i, j] += tmp\n",
    "    return out\n",
    "\n",
    "@jit\n",
    "def uint8_matmul_tiled(a: NDArray[np.uint8], b: NDArray[np.uint8]) -> NDArray[np.uint32]:\n",
    "    n, d = a.shape\n",
    "    d2, m = b.shape\n",
    "    assert (a < 16).all(), \"Large value will trigger multiplication overlfow\"\n",
    "    assert (b < 16).all(), \"Large value will trigger multiplication overlfow\"\n",
    "    assert d == d2\n",
    "    assert n % ks == 0\n",
    "    assert m % ks == 0\n",
    "    assert d % ks == 0\n",
    "    out = np.zeros((n, m), dtype=np.uint32)\n",
    "    n_chunks = n // ks\n",
    "    m_chunks = m // ks\n",
    "    d_chunks = d // ks\n",
    "    for ijp in range(n_chunks * m_chunks):\n",
    "        i, j = divmod(ijp, m_chunks)\n",
    "        i_start = i * ks\n",
    "        i_end = i_start + ks\n",
    "        j_start = j * ks\n",
    "        j_end = j_start + ks\n",
    "        out_chunk = out[i_start:i_end, j_start:j_end]\n",
    "        for k in range(d_chunks):\n",
    "            k_start = k * ks\n",
    "            k_end = k_start + ks\n",
    "            a_chunk = a[i_start:i_end, k_start:k_end]\n",
    "            b_chunk = b[k_start:k_end, j_start:j_end]\n",
    "            _mm_uint8_kernel(out_chunk, a_chunk, b_chunk)\n",
    "    return out\n",
    "\n",
    "def uint8_matmul_einsum_threaded(a: NDArray[np.uint8], b: NDArray[np.uint8], num_thread: int = 4) -> NDArray[np.uint32]:\n",
    "    n, d = a.shape\n",
    "    d2, m = b.shape\n",
    "\n",
    "    # Allocate output.\n",
    "    out = np.empty((n, m), dtype=np.uint32)\n",
    "\n",
    "    # Swap a and b via transposing if b is bigger, since we split on a.\n",
    "    transpose = m > n\n",
    "    if transpose:\n",
    "        tmp = a\n",
    "        a = b.T\n",
    "        b = tmp.T\n",
    "        out = out.T\n",
    "        n, d = a.shape\n",
    "        d2, m = b.shape\n",
    "\n",
    "    slice_size = n // num_thread\n",
    "    row_slices = [slice(start, start + slice_size) for start in range(0, n, slice_size)]\n",
    "\n",
    "    def _target(s):\n",
    "        out[s, :] = uint8_matmul_einsum(a[s, :], b)\n",
    "        \n",
    "    with ThreadPool(num_thread) as pool:\n",
    "        deque(pool.map(_target, row_slices), maxlen=0)\n",
    "\n",
    "    # Un-transpose if we transposed above.\n",
    "    if transpose:\n",
    "        out = out.T\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e168e917-4ebe-4295-af32-2f1bc2cb5a9e",
   "metadata": {},
   "source": [
    "## Trying to beat BLAS-based fp32 matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "600dfe33-e872-4a2b-8db0-800568a5bd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = (a.astype(np.float32) @ b.astype(np.float32)).astype(np.uint32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "890c500a-773c-4aa0-a24e-09f2a3527a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.8 ms ± 2.65 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "_ = (a.astype(np.float32) @ b.astype(np.float32)).astype(np.uint32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afc0465-ccee-43bb-9f34-e3d7620784ef",
   "metadata": {},
   "source": [
    "## Attempts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05be9ce9-3188-47bf-929f-ec0a1b8e25d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.all(uint8_matmul_cython(a, b) == ground_truth)\n",
    "assert np.all(uint8_matmul_numba(a, b) == ground_truth)\n",
    "assert np.all(uint8_matmul_einsum(a, b) == ground_truth)\n",
    "assert np.all(uint8_matmul_tiled(a, b) == ground_truth)\n",
    "assert np.all(uint8_matmul_einsum_threaded(a, b) == ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab5dd095-a256-4f9e-970c-feaaf946ac37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202 ms ± 989 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "_ = uint8_matmul_einsum(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec371ddc-9fd2-4b30-8d1d-290752983fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "358 ms ± 2.51 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "_ = uint8_matmul_numba(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2fb09c8-a7af-42be-b436-5bae69f8cd2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "381 ms ± 2.05 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "_ = uint8_matmul_tiled(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "093f7bf8-5950-4763-9eea-27ad44396a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.9 ms ± 1.72 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "_ = uint8_matmul_einsum_threaded(a, b, num_thread=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "679c722d-8151-418d-9544-895ac9340685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.5 ms ± 149 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "_ = uint8_matmul_cython(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4e315b-5b7c-4bd5-8543-d2b0342183b5",
   "metadata": {},
   "source": [
    "# A possibly easier challenge - vector-matrix multiplication\n",
    "\n",
    "In practice, search is often performed one query at a time, implying a (query embedding) vector vs. (document embedding) matrix multiplication. Let's see if we can accelerate this operation on uint8 datatypes to be competitive with the BLAS baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbfb5572-67ca-4f12-8fb8-e800033f5dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def uint8_vector_matrix_multiplication(mat: NDArray[np.uint8], vec: NDArray[np.uint8]) -> NDArray[np.uint32]:\n",
    "    \"\"\"Optimized multi-threaded implementation of matmul between uint4-stored-in-uint8 values.\"\"\"\n",
    "    assert mat.ndim == 2\n",
    "    assert vec.ndim == 1\n",
    "    n, d = mat.shape\n",
    "    (d2,) = vec.shape\n",
    "    assert d2 == d\n",
    "    assert (mat < 16).all(), \"Large value will trigger multiplication overlfow\"\n",
    "    assert (vec < 16).all(), \"Large value will trigger multiplication overlfow\"\n",
    "    out = np.empty(n, dtype=np.uint32)\n",
    "    for i in range(n):\n",
    "        row = mat[i, :]\n",
    "        tmp = np.uint32(0)\n",
    "        for j in range(d):\n",
    "            tmp += row[j] * vec[j]\n",
    "        out[i] = tmp\n",
    "    return out\n",
    "\n",
    "@jit\n",
    "def _mv_uint8_kernel(out: NDArray[np.uint32], mat: NDArray[np.uint8], vec: NDArray[np.uint8]) -> NDArray[np.uint32]:\n",
    "    for i in range(ks):\n",
    "        row = mat[i, :]\n",
    "        for k in range(ks):\n",
    "            out[i] += row[k] * vec[k]\n",
    "    return out\n",
    "\n",
    "@jit\n",
    "def mv_uint8_tiled(mat: NDArray[np.uint8], vec: NDArray[np.uint8]) -> NDArray[np.uint32]:\n",
    "    assert mat.ndim == 2\n",
    "    assert vec.ndim == 1\n",
    "    n, d = mat.shape\n",
    "    (d2,) = vec.shape\n",
    "    assert d2 == d\n",
    "    assert (mat < 16).all(), \"Large value will trigger multiplication overlfow\"\n",
    "    assert (vec < 16).all(), \"Large value will trigger multiplication overlfow\"\n",
    "    assert n % ks == 0\n",
    "    assert d % ks == 0\n",
    "    out = np.zeros(n, dtype=np.uint32)\n",
    "    n_chunks = n // ks\n",
    "    d_chunks = d // ks\n",
    "    for j in range(d_chunks):\n",
    "        j_start = j * ks\n",
    "        j_end = j_start + ks\n",
    "        vec_chunk = vec[j_start:j_end]\n",
    "        for i in range(n_chunks):\n",
    "            i_start = i * ks\n",
    "            i_end = i_start + ks\n",
    "            mat_chunk = mat[i_start:i_end, j_start:j_end]\n",
    "            out_chunk = out[i_start:i_end]\n",
    "            _mv_uint8_kernel(out_chunk, mat_chunk, vec_chunk)\n",
    "    return out\n",
    "\n",
    "\n",
    "def mat_vec_einsum(mat: NDArray[np.uint8], vec: NDArray[np.uint8]) -> NDArray[np.uint32]:\n",
    "    return np.einsum(\"ij, j -> i\", mat, vec, dtype=np.uint32)\n",
    "\n",
    "\n",
    "\n",
    "def mat_vec_einsum_multithread(mat: NDArray[np.uint8], vec: NDArray[np.uint8]) -> NDArray[np.uint32]:\n",
    "    n, d = mat.shape\n",
    "    num_thread = cpu_count()\n",
    "\n",
    "    # Split the rows of the matrix across worker threads to dispatch to multiple CPU cores.\n",
    "    slice_size = n // num_thread\n",
    "    mat_row_slices = [slice(start, start + slice_size) for start in range(0, n, slice_size)]\n",
    "    out = np.empty(n, dtype=np.uint32)\n",
    "\n",
    "    def _target(s):\n",
    "        out[s] = mat_vec_einsum(mat[s, :], vec)\n",
    "        \n",
    "    with ThreadPool(num_thread) as pool:\n",
    "        deque(pool.map(_target, mat_row_slices), maxlen=0)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c27ad3f-cf36-4684-a3e7-6be587cee968",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_matvec = 1024 * 1024\n",
    "d_matvec = 256\n",
    "mat = rng.choice(15, (m_matvec, d_matvec)).astype(np.uint8)\n",
    "vec = rng.choice(15, (d_matvec,)).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "927496cc-459d-4ea5-b95f-749732cccc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_gt = (mat.astype(np.float32) @ vec.astype(np.float32)).astype(np.uint32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9caf663c-dee0-42d5-9b4c-1e52c2a91912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.4 ms ± 1.92 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "_ = (mat.astype(np.float32) @ vec.astype(np.float32)).astype(np.uint32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "051f7653-a435-48d6-a830-2b34f1643793",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.all(uint8_vector_matrix_multiplication(mat, vec) == mv_gt)\n",
    "assert np.all(mv_uint8_tiled(mat, vec) == mv_gt)\n",
    "assert np.all(mat_vec_einsum(mat, vec) == mv_gt)\n",
    "assert np.all(mat_vec_einsum_multithread(mat, vec) == mv_gt)\n",
    "assert np.all(uint8_matmul_cython(vec[None, :], mat.T) == mv_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d9adf1e-339d-4805-a82c-0de36243db27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "424 ms ± 1.67 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "uint8_vector_matrix_multiplication(mat, vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba50eddb-3d8e-4eab-952f-d8878eb1a70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "629 ms ± 4.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "mv_uint8_tiled(mat, vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8931aa02-23d4-4fee-8d76-1b6d374bd0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.5 ms ± 315 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "mat_vec_einsum(mat, vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c272e830-41b1-4be4-9e63-38c60df93089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.9 ms ± 284 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "mat_vec_einsum_multithread(mat, vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b1bcdcd8-7734-4c2e-a540-83998fdc4e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.8 ms ± 135 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "uint8_matmul_cython(vec[None, :], mat.T).T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
