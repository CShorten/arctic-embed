{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc2a0890-910e-4dca-9471-f80974931ab3",
   "metadata": {},
   "source": [
    "# This is a playground notebook\n",
    "\n",
    "In order to evaluate int4 compression, we spent some time trying to get a decently fast matrix multiply operation implemented using `numpy` and `numba`. Unfortunately it was quite hard to compete with the performance of built-in `matmul` in `numpy` due to that function relying on very well optimized BLAS routines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce61c9f1-5404-4930-9e3d-efce115a6271",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import cpu_count\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from collections import deque\n",
    "\n",
    "import numba\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "jit = numba.njit(error_model=\"numpy\", fastmath=True)\n",
    "ks = 16  # Kernel size.\n",
    "\n",
    "# Test data.\n",
    "n, m, d = 256, 4 * 4096, 256\n",
    "rng = np.random.default_rng(0)\n",
    "a = rng.choice(15, (n, d)).astype(np.uint8)\n",
    "b = rng.choice(15, (m, d)).astype(np.uint8).T\n",
    "\n",
    "@jit\n",
    "def uint8_matmul(a: NDArray[np.uint8], b: NDArray[np.uint8]) -> NDArray[np.uint32]:\n",
    "    \"\"\"Optimized multi-threaded implementation of matmul between uint4-stored-in-uint8 values.\"\"\"\n",
    "    n, d = a.shape\n",
    "    d2, m = b.shape\n",
    "    assert d2 == d\n",
    "    assert (a < 16).all(), \"Large value will trigger multiplication overlfow\"\n",
    "    assert (b < 16).all(), \"Large value will trigger multiplication overlfow\"\n",
    "    out = np.empty((n, m), dtype=np.uint32)\n",
    "    for i in range(n):\n",
    "        row = a[i, :]\n",
    "        for j in range(m):\n",
    "            col = b[:, j]\n",
    "            tmp = np.uint32(0)\n",
    "            for k in range(d):\n",
    "                tmp += row[k] * col[k]\n",
    "            out[i, j] = tmp\n",
    "    return out\n",
    "\n",
    "@jit\n",
    "def _mm_uint8_kernel(out: NDArray[np.uint32], a: NDArray[np.uint8], b: NDArray[np.uint8]) -> NDArray[np.uint32]:\n",
    "    for i in range(ks):\n",
    "        row = a[i, :]\n",
    "        for j in range(ks):\n",
    "            col = b[:, j]\n",
    "            tmp = np.uint32(0)\n",
    "            for k in range(ks):\n",
    "                tmp += row[k] * col[k]\n",
    "            out[i, j] += tmp\n",
    "    return out\n",
    "\n",
    "@jit\n",
    "def mm_uint8_tiled(a: NDArray[np.uint8], b: NDArray[np.uint8]) -> NDArray[np.uint32]:\n",
    "    n, d = a.shape\n",
    "    d2, m = b.shape\n",
    "    assert (a < 16).all(), \"Large value will trigger multiplication overlfow\"\n",
    "    assert (b < 16).all(), \"Large value will trigger multiplication overlfow\"\n",
    "    assert d == d2\n",
    "    assert n % ks == 0\n",
    "    assert m % ks == 0\n",
    "    assert d % ks == 0\n",
    "    out = np.zeros((n, m), dtype=np.uint32)\n",
    "    n_chunks = n // ks\n",
    "    m_chunks = m // ks\n",
    "    d_chunks = d // ks\n",
    "    for ijp in range(n_chunks * m_chunks):\n",
    "        i, j = divmod(ijp, m_chunks)\n",
    "        i_start = i * ks\n",
    "        i_end = i_start + ks\n",
    "        j_start = j * ks\n",
    "        j_end = j_start + ks\n",
    "        out_chunk = out[i_start:i_end, j_start:j_end]\n",
    "        for k in range(d_chunks):\n",
    "            k_start = k * ks\n",
    "            k_end = k_start + ks\n",
    "            a_chunk = a[i_start:i_end, k_start:k_end]\n",
    "            b_chunk = b[k_start:k_end, j_start:j_end]\n",
    "            _mm_uint8_kernel(out_chunk, a_chunk, b_chunk)\n",
    "    return out\n",
    "\n",
    "\n",
    "def mm_einsum(a: NDArray[np.uint8], b: NDArray[np.uint8]) -> NDArray[np.uint32]:\n",
    "    return np.einsum(\"ij, jk -> ik\", a, b, dtype=np.uint32)\n",
    "\n",
    "def mm_einsum_multithread(a: NDArray[np.uint8], b: NDArray[np.uint8], num_thread: int = 4) -> NDArray[np.uint32]:\n",
    "    n, d = a.shape\n",
    "    d2, m = b.shape\n",
    "    num_thread = cpu_count()\n",
    "\n",
    "    # Allocate output.\n",
    "    out = np.empty((n, m), dtype=np.uint32)\n",
    "\n",
    "    # Swap a and b via transposing if b is bigger, since we split on a.\n",
    "    transpose = m > n\n",
    "    if transpose:\n",
    "        tmp = a\n",
    "        a = b.T\n",
    "        b = tmp.T\n",
    "        out = out.T\n",
    "        n, d = a.shape\n",
    "        d2, m = b.shape\n",
    "\n",
    "    slice_size = n // num_thread\n",
    "    row_slices = [slice(start, start + slice_size) for start in range(0, n, slice_size)]\n",
    "\n",
    "    def _target(s):\n",
    "        out[s, :] = mm_einsum(a[s, :], b)\n",
    "        \n",
    "    with ThreadPool(num_thread) as pool:\n",
    "        deque(pool.map(_target, row_slices), maxlen=0)\n",
    "\n",
    "    # Un-transpose if we transposed above.\n",
    "    if transpose:\n",
    "        out = out.T\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e168e917-4ebe-4295-af32-2f1bc2cb5a9e",
   "metadata": {},
   "source": [
    "## Trying to beat BLAS-based fp32 matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "600dfe33-e872-4a2b-8db0-800568a5bd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = (a.astype(np.float32) @ b.astype(np.float32)).astype(np.uint32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "890c500a-773c-4aa0-a24e-09f2a3527a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.8 ms ± 2.71 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "_ = (a.astype(np.float32) @ b.astype(np.float32)).astype(np.uint32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afc0465-ccee-43bb-9f34-e3d7620784ef",
   "metadata": {},
   "source": [
    "## Attempts\n",
    "(note: mostly single-threaded, so even being 1/10 as fast would be nice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05be9ce9-3188-47bf-929f-ec0a1b8e25d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "o1 = uint8_matmul(a, b)\n",
    "assert np.all(o1 == ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5edc244-62da-455c-b4af-b66080716594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "357 ms ± 4.57 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "_ = uint8_matmul(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10fff9a4-5235-4c5e-a670-ee724f9ab882",
   "metadata": {},
   "outputs": [],
   "source": [
    "o2 = mm_uint8_tiled(a, b)\n",
    "assert np.all(o2 == ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36a228bc-edc4-47b9-a222-ad3f40e1d144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "377 ms ± 3.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "_ = mm_uint8_tiled(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f19255a-e36f-418e-8a03-586b2e8d34fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "o3 = mm_einsum(a, b)\n",
    "assert np.all(o3 == ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfae33a5-8517-4740-a889-3ba9900f0796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201 ms ± 2.41 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "_ = mm_einsum(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88c7ff6d-2a4e-4371-8ffe-36902c2dc465",
   "metadata": {},
   "outputs": [],
   "source": [
    "o4 = mm_einsum_multithread(a, b)\n",
    "assert np.all(o4 == ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64606c58-7fee-4180-84e9-6039c9eda4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.5 ms ± 2.28 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "_ = mm_einsum_multithread(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4e315b-5b7c-4bd5-8543-d2b0342183b5",
   "metadata": {},
   "source": [
    "# A possibly easier challenge - vector-matrix multiplication\n",
    "\n",
    "In practice, search is often performed one query at a time, implying a (query embedding) vector vs. (document embedding) matrix multiplication. Let's see if we can accelerate this operation on uint8 datatypes to be competitive with the BLAS baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbfb5572-67ca-4f12-8fb8-e800033f5dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def uint8_vector_matrix_multiplication(mat: NDArray[np.uint8], vec: NDArray[np.uint8]) -> NDArray[np.uint32]:\n",
    "    \"\"\"Optimized multi-threaded implementation of matmul between uint4-stored-in-uint8 values.\"\"\"\n",
    "    assert mat.ndim == 2\n",
    "    assert vec.ndim == 1\n",
    "    n, d = mat.shape\n",
    "    (d2,) = vec.shape\n",
    "    assert d2 == d\n",
    "    assert (mat < 16).all(), \"Large value will trigger multiplication overlfow\"\n",
    "    assert (vec < 16).all(), \"Large value will trigger multiplication overlfow\"\n",
    "    out = np.empty(n, dtype=np.uint32)\n",
    "    for i in range(n):\n",
    "        row = mat[i, :]\n",
    "        tmp = np.uint32(0)\n",
    "        for j in range(d):\n",
    "            tmp += row[j] * vec[j]\n",
    "        out[i] = tmp\n",
    "    return out\n",
    "\n",
    "@jit\n",
    "def _mv_uint8_kernel(out: NDArray[np.uint32], mat: NDArray[np.uint8], vec: NDArray[np.uint8]) -> NDArray[np.uint32]:\n",
    "    for i in range(ks):\n",
    "        row = mat[i, :]\n",
    "        for k in range(ks):\n",
    "            out[i] += row[k] * vec[k]\n",
    "    return out\n",
    "\n",
    "@jit\n",
    "def mv_uint8_tiled(mat: NDArray[np.uint8], vec: NDArray[np.uint8]) -> NDArray[np.uint32]:\n",
    "    assert mat.ndim == 2\n",
    "    assert vec.ndim == 1\n",
    "    n, d = mat.shape\n",
    "    (d2,) = vec.shape\n",
    "    assert d2 == d\n",
    "    assert (mat < 16).all(), \"Large value will trigger multiplication overlfow\"\n",
    "    assert (vec < 16).all(), \"Large value will trigger multiplication overlfow\"\n",
    "    assert n % ks == 0\n",
    "    assert d % ks == 0\n",
    "    out = np.zeros(n, dtype=np.uint32)\n",
    "    n_chunks = n // ks\n",
    "    d_chunks = d // ks\n",
    "    for j in range(d_chunks):\n",
    "        j_start = j * ks\n",
    "        j_end = j_start + ks\n",
    "        vec_chunk = vec[j_start:j_end]\n",
    "        for i in range(n_chunks):\n",
    "            i_start = i * ks\n",
    "            i_end = i_start + ks\n",
    "            mat_chunk = mat[i_start:i_end, j_start:j_end]\n",
    "            out_chunk = out[i_start:i_end]\n",
    "            _mv_uint8_kernel(out_chunk, mat_chunk, vec_chunk)\n",
    "    return out\n",
    "\n",
    "\n",
    "def mat_vec_einsum(mat: NDArray[np.uint8], vec: NDArray[np.uint8]) -> NDArray[np.uint32]:\n",
    "    return np.einsum(\"ij, j -> i\", mat, vec, dtype=np.uint32)\n",
    "\n",
    "\n",
    "\n",
    "def mat_vec_einsum_multithread(mat: NDArray[np.uint8], vec: NDArray[np.uint8]) -> NDArray[np.uint32]:\n",
    "    n, d = mat.shape\n",
    "    num_thread = cpu_count()\n",
    "\n",
    "    # Split the rows of the matrix across worker threads to dispatch to multiple CPU cores.\n",
    "    slice_size = n // num_thread\n",
    "    mat_row_slices = [slice(start, start + slice_size) for start in range(0, n, slice_size)]\n",
    "    out = np.empty(n, dtype=np.uint32)\n",
    "\n",
    "    def _target(s):\n",
    "        out[s] = mat_vec_einsum(mat[s, :], vec)\n",
    "        \n",
    "    with ThreadPool(num_thread) as pool:\n",
    "        deque(pool.map(_target, mat_row_slices), maxlen=0)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c27ad3f-cf36-4684-a3e7-6be587cee968",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_matvec = 1024 * 1024\n",
    "d_matvec = 256\n",
    "mat = rng.choice(15, (m_matvec, d_matvec)).astype(np.uint8)\n",
    "vec = rng.choice(15, (d_matvec,)).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "927496cc-459d-4ea5-b95f-749732cccc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_gt = (mat.astype(np.float32) @ vec.astype(np.float32)).astype(np.uint32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9caf663c-dee0-42d5-9b4c-1e52c2a91912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66.7 ms ± 3.98 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "_ = (mat.astype(np.float32) @ vec.astype(np.float32)).astype(np.uint32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d9adf1e-339d-4805-a82c-0de36243db27",
   "metadata": {},
   "outputs": [],
   "source": [
    "o_mv1 = uint8_vector_matrix_multiplication(mat, vec)\n",
    "assert np.all(o_mv1 == mv_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ad40d77-96a3-43af-b39c-3cbe9bef4278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "437 ms ± 5.06 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "_ = uint8_vector_matrix_multiplication(mat, vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4da78651-7549-479b-a791-6cbafc978e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "o_mv2 = mv_uint8_tiled(mat, vec)\n",
    "assert np.all(o_mv2 == mv_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca2e18e3-a780-4c86-b1bf-7af26eb9c97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "626 ms ± 6.38 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "_ = mv_uint8_tiled(mat, vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e90437dc-a8a7-4e19-a7bf-fe5b0b969e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "o_mv3 = mat_vec_einsum(mat, vec)\n",
    "assert np.all(o_mv3 == mv_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d94a09b6-6c5c-438f-b515-d2d08b79692a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.2 ms ± 354 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "_ = mat_vec_einsum(mat, vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c67733db-3897-4c04-899e-98685c94ff0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "o_mv4 = mat_vec_einsum_multithread(mat, vec)\n",
    "assert np.all(o_mv4 == mv_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da0169c5-b735-4b6d-b5bc-36c3f669ee47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.4 ms ± 483 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "_ = mat_vec_einsum_multithread(mat, vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c614e7-a9ee-4523-b372-f69f1c6cac76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
